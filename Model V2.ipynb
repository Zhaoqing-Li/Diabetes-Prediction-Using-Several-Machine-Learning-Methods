{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import  LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files from different datasets\n",
    "PIMA=pd.read_csv('PIMA_diabetes_preprocessed.csv')\n",
    "Sylhet=pd.read_csv('Sylhet_diabetes_preprocessed.csv')\n",
    "Mendeley=pd.read_csv('Mendeley_diabetes_preprocessed.csv')\n",
    "Azure=pd.read_csv('Azure_diabetes_preprocessed.csv')\n",
    "Fused=pd.read_csv('Fused_diabetes.csv')\n",
    "BRFSS=pd.read_csv('BRFSS_diabetes_preprocessed.csv')\n",
    "BRFSS_Demo=pd.read_csv('BRFSS_Demo.csv')\n",
    "\n",
    "# save all datasets into a dictionary\n",
    "datasets = {'PIMA': PIMA, 'Sylhet': Sylhet, 'Mendeley': Mendeley, 'Azure': Azure,\n",
    "             'Fused': Fused, 'BRFSS': BRFSS, 'BRFSS_Demo': BRFSS_Demo}\n",
    "\n",
    "# Since the gender is the top variance in Mendeley and Azure datasets\n",
    "# Here the Mendeley and Azure will be separated into two datasets depending on the gender.\n",
    "# PCA and model fitting will be applied on both separated datasets\n",
    "Mendeley_Male = Mendeley[Mendeley['Gender'] == 1]\n",
    "Mendeley_Female = Mendeley[Mendeley['Gender'] == 2]\n",
    "Azure_Male = Azure[Azure['SEX'] == 1]\n",
    "Azure_Female = Azure[Azure['SEX'] == 2]\n",
    "\n",
    "# Save four new datasets into the dictionary\n",
    "datasets['Mendeley_Male'] = Mendeley_Male\n",
    "datasets['Mendeley_Female'] = Mendeley_Female\n",
    "datasets['Azure_Male'] = Azure_Male\n",
    "datasets['Azure_Female'] = Azure_Female\n",
    "\n",
    "# Since glucose is the indicator of diabetes, this feature should be dropped for diabetes prediction.\n",
    "PIMA_NoGlucose = PIMA.drop('Glucose', axis=1)\n",
    "Mendeley_NoGlucose = Mendeley.drop('HbA1c', axis=1)\n",
    "Fused_NoGlucose = Fused.drop('Glucose', axis=1)\n",
    "\n",
    "# Save the datasets without Glucose\n",
    "datasets['PIMA_NoGlucose'] = PIMA_NoGlucose\n",
    "datasets['Mendeley_NoGlucose'] = Mendeley_NoGlucose\n",
    "datasets['Fused_NoGlucose'] = Fused_NoGlucose\n",
    "\n",
    "print(Mendeley_NoGlucose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for Principal Components Analysis and visualization\n",
    "def mypca(n_componets = 5, dataset_name = 'PIMA'):\n",
    "\n",
    "    # Prepare the training and test sets\n",
    "    dataset = datasets[dataset_name]\n",
    "    X = dataset.drop([\"Class\"], axis = 1)\n",
    "    scaler = MinMaxScaler() \n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(n_components=n_componets) # keep the first 5 principal components of the data\n",
    "    X_pca = pca.fit_transform(X) # Fit PCA on the scaled features\n",
    "\n",
    "    # visualize the feature space\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title(f'Explained Variance by Components ({dataset_name})')\n",
    "    plt.show()\n",
    "\n",
    "    # visualize the features with loadings\n",
    "    features = dataset.drop([\"Class\"], axis=1).columns\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'], index=features)\n",
    "    pc1_loadings = loadings['PC1'].abs().sort_values(ascending=False) # Find the first principal component\n",
    "\n",
    "    # Select the top 5 features\n",
    "    top_features = pc1_loadings.head(5).index.tolist()\n",
    "    top_loadings = pc1_loadings.head(5).values\n",
    "\n",
    "    # use a bar plot to visualize the features\n",
    "    plt.bar(top_features, top_loadings)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Loading')\n",
    "    plt.title(f'Top 5 Features for PC1 ({dataset_name})')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA for feature selection for all four datasets\n",
    "# mypca(n_componets = 5, dataset_name = 'PIMA')\n",
    "# mypca(n_componets = 5, dataset_name = 'Sylhet')\n",
    "# mypca(n_componets = 5, dataset_name = 'Mendeley')\n",
    "# mypca(n_componets = 5, dataset_name = 'Azure')\n",
    "# mypca(n_componets=5, dataset_name='Mendeley_Male')\n",
    "# mypca(n_componets=5, dataset_name='Mendeley_Female')\n",
    "# mypca(n_componets=5, dataset_name='Azure_Male')\n",
    "# mypca(n_componets=5, dataset_name='Azure_Female')\n",
    "mypca(n_componets = 5, dataset_name = 'BRFSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switches for different models\n",
    "RF = False\n",
    "LR = False\n",
    "SVM = True\n",
    "DT = False\n",
    "XG = False\n",
    "balancing = False\n",
    "\n",
    "# Set the dataset\n",
    "dataset = datasets['BRFSS_Demo']\n",
    "\n",
    "# Check the dataset\n",
    "y=dataset['Class']\n",
    "percent_pos = sum(y)/len(y)\n",
    "print('Percentage Diabetes cases %.02f %%' %(percent_pos * 100))\n",
    "\n",
    "# Split the data into features and classes\n",
    "X = dataset.drop([\"Class\"], axis = 1)\n",
    "y = dataset[\"Class\"]\n",
    "\n",
    "# Normalize the features data\n",
    "scaler = MinMaxScaler() \n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create tuned model with parameter grid\n",
    "if RF:    \n",
    "    model = RandomForestClassifier(random_state=123, n_jobs=-1) # use all processors for calculation\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "    'smote__k_neighbors': [1, 5, 10],  # SMOTE parameters\n",
    "    'classifier__n_estimators': [100, 200, 300],  # RF parameters\n",
    "    'classifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'classifier__max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    \n",
    "if LR:\n",
    "    model = LogisticRegression(random_state=123)\n",
    "\n",
    "if SVM:\n",
    "    model = SVC(random_state=123, cache_size=2000)\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "    'smote__k_neighbors': [1, 5, 10],  # SMOTE parameters\n",
    "    'classifier__C': [1, 10, 100],  # The strength of the regularization is inversely proportional to C.\n",
    "    'classifier__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    # 'classifier__gamma': ['scale', 'auto'], # scale: 1 / (n_features * X.var()); auto: 1 / n_features\n",
    "    # 'classifier__degree': [1, 3, 5], # relevant for the 'poly' kernel\n",
    "    'classifier__coef0': [0.0, 0.5, 1.0] # significant for 'poly' and 'sigmoid' kernels\n",
    "    }\n",
    "\n",
    "if DT:\n",
    "    model = DecisionTreeClassifier(random_state=123)\n",
    "    \n",
    "if XG:\n",
    "    model = XGBClassifier(tree_method='hist', device='cpu') # use hist tree and run on all threads of CPU\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'smote__k_neighbors': [1, 5, 10],  # SMOTE parameters\n",
    "        'classifier__eta': [0.1, 0.3, 0.9],  # lower eta makes the boosting process more conservative\n",
    "        'classifier__gamma': [0, 1, 10],  # The larger gamma is, the more conservative the algorithm will be.\n",
    "        'classifier__max_depth': [0, 6, 12],  # Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit on depth.\n",
    "        'classifier__min_child_weight': [0, 1, 2],  # The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "        'classifier__subsample': [0.1, 0.5, 1], # Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting.\n",
    "        # 'classifier__lambda': [0, 1, 10], # Increasing this value will make model more conservative.\n",
    "        # 'classifier__alpha': [0, 1, 10], # Increasing this value will make model more conservative.\n",
    "        # 'classifier__refresh_leaf': [0, 1],\n",
    "        # 'classifier__grow_policy': ['depthwise', 'lossguide'],\n",
    "        # 'classifier__max_bin': [128, 256, 512]\n",
    "    }\n",
    "\n",
    "if balancing:\n",
    "    smote = SMOTE(random_state=123)\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([('smote', smote), ('classifier', model)])\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=10, scoring='roc_auc') # 10 folds cross-validation, AUC for evaluation\n",
    "    \n",
    "    # start the grid search and record the computaion time\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X,y)\n",
    "    spend_time = time.time()-start_time\n",
    "    \n",
    "    #Print the best parameters and AUC score\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "    print(\"Spend time:\", spend_time)\n",
    "\n",
    "else:\n",
    "    pipeline = Pipeline([('classifier', model)])\n",
    "\n",
    "    # Remove the first key-value pair to skip the SMOTE\n",
    "    next(iter(param_grid)) \n",
    "    param_grid.pop(next(iter(param_grid)))\n",
    "    \n",
    "    # Same as above\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=10, scoring='roc_auc')\n",
    "\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X,y)\n",
    "    spend_time = time.time()-start_time\n",
    "\n",
    "    # print as above\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "    print(\"Spend time:\", spend_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(model = 'RF', data = 'PIMA', k_neighbors_smote = 0, \n",
    "                     criterion_rf = 'entropy', max_features_rf = 'log2', n_estimators_rf = 600, \n",
    "                     C_svm = 0.1, coef0_svm = 0.0, degree_svm = 3, gamma_svm = 'scale', kernel_svm = 'poly', \n",
    "                     eta_xg = 0.3, gamma_xg = 10, max_depth_xg = 0, min_child_weight_xg = 2, subsample_xg = 1):\n",
    "    '''This function reads the dataset and use RF, SVM, or XG Boost model with passed parameters to traning and fit with it. \n",
    "    The process will repeat 100 times to get the average AUC and F1 score, as well as the 95% CI'''\n",
    "    \n",
    "    # Avoid name duplication\n",
    "    df = datasets[data]\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    X = df.drop(['Class'], axis=1)\n",
    "    y = df['Class']\n",
    "\n",
    "    # save the features\n",
    "    features = X.columns\n",
    "\n",
    "    # Normalize the features data\n",
    "    scaler = MinMaxScaler() \n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize lists to store scores\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Repeat the process 100 times\n",
    "    for _ in range(100):\n",
    "        # Split the data randomly each time\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "        if k_neighbors_smote !=0:\n",
    "            smote = SMOTE(random_state=123, k_neighbors=k_neighbors_smote)\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "        # Initialize the model Classifier\n",
    "        if model == 'RF':\n",
    "            clf = RandomForestClassifier(random_state=123, n_jobs=-1, criterion=criterion_rf, max_features=max_features_rf, n_estimators=n_estimators_rf)\n",
    "        if model == 'SVM':\n",
    "            clf = SVC(random_state=123, cache_size=2000, C=C_svm, coef0=coef0_svm, degree=degree_svm, gamma=gamma_svm, kernel=kernel_svm)\n",
    "        if model == 'XG':\n",
    "            clf = XGBClassifier(tree_method='hist', device='cpu', eta=eta_xg, gamma=gamma_xg, max_depth=max_depth_xg, min_child_weight=min_child_weight_xg, subsample=subsample_xg)\n",
    "    \n",
    "        # Fit the model\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "        # Predict on the test set\n",
    "        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "        y_pred = clf.predict(X_test)\n",
    "    \n",
    "        # Calculate scores and append to lists\n",
    "        auc_scores.append(roc_auc_score(y_test, y_pred_proba))\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    # Calculate mean and 95% CI for AUC and F1 score\n",
    "    auc_mean = np.mean(auc_scores)\n",
    "    f1_mean = np.mean(f1_scores)\n",
    "    auc_ci = stats.t.interval(0.95, len(auc_scores)-1, loc=auc_mean, scale=stats.sem(auc_scores))\n",
    "    f1_ci = stats.t.interval(0.95, len(f1_scores)-1, loc=f1_mean, scale=stats.sem(f1_scores))\n",
    "\n",
    "    # Identify top 5 most important features\n",
    "    feature_importances = clf.feature_importances_\n",
    "    top_features_indices = feature_importances.argsort()[-5:][::-1]  # Get indices of top 5 features\n",
    "    top_features_names = features[top_features_indices]\n",
    "    top_features_importances = feature_importances[top_features_indices]\n",
    "\n",
    "    print(f'AUC Mean: {auc_mean}, 95% CI: {auc_ci}')\n",
    "    print(f'F1 Mean: {f1_mean}, 95% CI: {f1_ci}')\n",
    "    print(f'Top 5 Most Important Features in {model} model ({data}):')\n",
    "    for name, importance in zip(top_features_names, top_features_importances):\n",
    "        print(f'{name}: {importance}')\n",
    "\n",
    "    # Create a bar plot for the top 5 features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(top_features_names, top_features_importances, color='skyblue')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title(f'Top 5 Most Important Features in {model} model ({data})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_validation(model = 'RF', data='PIMA', k_neighbors_smote=2, criterion_rf='entropy', max_features_rf='log2', n_estimators_rf=600)\n",
    "# model_validation(model = 'RF', data='PIMA', k_neighbors_smote=0, criterion_rf='entropy', max_features_rf='log2', n_estimators_rf=500)\n",
    "# model_validation(model = 'RF', data='Sylhet', k_neighbors_smote=1, criterion_rf='gini', max_features_rf='sqrt', n_estimators_rf=400)\n",
    "# model_validation(model = 'RF', data='Sylhet', k_neighbors_smote=0, criterion_rf='entropy', max_features_rf='sqrt', n_estimators_rf=500)\n",
    "# model_validation(model = 'RF', data='Mendeley', k_neighbors_smote=1, criterion_rf='gini', max_features_rf='sqrt', n_estimators_rf=100)\n",
    "# model_validation(model = 'RF', data='Mendeley', k_neighbors_smote=0, criterion_rf='gini', max_features_rf='sqrt', n_estimators_rf=200)\n",
    "# model_validation(model = 'RF', data='Mendeley_Male', k_neighbors_smote=1, criterion_rf='entropy', max_features_rf='sqrt', n_estimators_rf=100)\n",
    "# model_validation(model = 'RF', data='Mendeley_Male', k_neighbors_smote=0, criterion_rf='gini', max_features_rf='sqrt', n_estimators_rf=300)\n",
    "# model_validation(model = 'RF', data='Mendeley_Female', k_neighbors_smote=1, criterion_rf='gini', max_features_rf='sqrt', n_estimators_rf=100)\n",
    "# model_validation(model = 'RF', data='Mendeley_Female', k_neighbors_smote=0, criterion_rf='gini', max_features_rf='sqrt', n_estimators_rf=100)\n",
    "# model_validation(model = 'RF', data='Fused', k_neighbors_smote=10, criterion_rf='entropy', max_features_rf='sqrt', n_estimators_rf=300)\n",
    "# model_validation(model = 'RF', data='Fused', k_neighbors_smote=0, criterion_rf='entropy', max_features_rf='sqrt', n_estimators_rf=300)\n",
    "# model_validation(model = 'RF', data='BRFSS_Demo', k_neighbors_smote=5, criterion_rf='entropy', max_features_rf='sqrt', n_estimators_rf=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_validation(model = 'XG', data='PIMA', k_neighbors_smote=5, eta_xg=0.3, gamma_xg=10, max_depth_xg=0, min_child_weight_xg=2, subsample_xg=1)\n",
    "# model_validation(model = 'XG', data='PIMA', k_neighbors_smote=0, eta_xg=0.1, gamma_xg=10, max_depth_xg=0, min_child_weight_xg=0, subsample_xg=1)\n",
    "# model_validation(model = 'XG', data='PIMA_NoGlucose', k_neighbors_smote=5, eta_xg=0.1, gamma_xg=10, max_depth_xg=0, min_child_weight_xg=2, subsample_xg=0.5)\n",
    "# model_validation(model = 'XG', data='PIMA_NoGlucose', k_neighbors_smote=0, eta_xg=0.1, gamma_xg=1, max_depth_xg=6, min_child_weight_xg=2, subsample_xg=1)\n",
    "# model_validation(model = 'XG', data='Sylhet', k_neighbors_smote=10, eta_xg=0.9, gamma_xg=0, max_depth_xg=6, min_child_weight_xg=0, subsample_xg=0.5)\n",
    "# model_validation(model = 'XG', data='Sylhet', k_neighbors_smote=0, eta_xg=0.3, gamma_xg=0, max_depth_xg=0, min_child_weight_xg=0, subsample_xg=0.5)\n",
    "# model_validation(model = 'XG', data='Mendeley', k_neighbors_smote=1, eta_xg=0.3, gamma_xg=10, max_depth_xg=0, min_child_weight_xg=2, subsample_xg=1)\n",
    "# model_validation(model = 'XG', data='Mendeley', k_neighbors_smote=0, eta_xg=0.1, gamma_xg=1, max_depth_xg=0, min_child_weight_xg=0, subsample_xg=0.5)\n",
    "# model_validation(model = 'XG', data='Mendeley_NoGlucose', k_neighbors_smote=0, eta_xg=0.1, gamma_xg=0, max_depth_xg=0, min_child_weight_xg=2, subsample_xg=0.5)\n",
    "# model_validation(model = 'XG', data='Mendeley_Male', k_neighbors_smote=5, eta_xg=0.1, gamma_xg=0, max_depth_xg=0, min_child_weight_xg=2, subsample_xg=1)\n",
    "# model_validation(model = 'XG', data='Mendeley_Male', k_neighbors_smote=0, eta_xg=0.9, gamma_xg=0, max_depth_xg=0, min_child_weight_xg=1, subsample_xg=1)\n",
    "# model_validation(model = 'XG', data='Mendeley_Female', k_neighbors_smote=1, eta_xg=0.1, gamma_xg=0, max_depth_xg=0, min_child_weight_xg=0, subsample_xg=0.5)\n",
    "# model_validation(model = 'XG', data='Mendeley_Female', k_neighbors_smote=0, eta_xg=0.1, gamma_xg=0, max_depth_xg=0, min_child_weight_xg=0, subsample_xg=0.5)\n",
    "# model_validation(model = 'XG', data='Fused_NoGlucose', k_neighbors_smote=1, eta_xg=0.3, gamma_xg=10, max_depth_xg=0, min_child_weight_xg=0, subsample_xg=0.5)\n",
    "model_validation(model = 'XG', data='Fused_NoGlucose', k_neighbors_smote=0, eta_xg=0.1, gamma_xg=10, max_depth_xg=0, min_child_weight_xg=0, subsample_xg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the best model from Fused NoGlucose to the BRFSS dataset\n",
    "df_train = Fused_NoGlucose\n",
    "\n",
    "X_train = df_train.drop(['Class'], axis=1)\n",
    "y_train = df_train['Class']\n",
    "\n",
    "model = XGBClassifier(tree_method='hist', device='cpu', eta=0.1, gamma=10, max_depth=0, min_child_weight=0, subsample=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "df_test = BRFSS\n",
    "\n",
    "# Change the name of 'Sex' to 'Gender'\n",
    "df_test.rename(columns={'Sex': 'Gender'}, inplace=True)\n",
    "\n",
    "# Set 0 in 'Gender' column as 1, and 1 in 'Gender' column as 2\n",
    "df_test['Gender'] = df_test['Gender'].replace({0: 2, 1: 1})\n",
    "\n",
    "# Prepare X_test and y_test\n",
    "X_test = df_test[['Gender', 'Age', 'BMI']]\n",
    "y_test = df_test['Class']\n",
    "\n",
    "# Making predictions\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "y_pred = model.predict(X_test)  # Get class predictions\n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1_score_result = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC: {auc_score}\")\n",
    "print(f\"F1 Score: {f1_score_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
